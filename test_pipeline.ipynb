{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.cluster import KMeans, OPTICS, DBSCAN, SpectralClustering\n",
    "\n",
    "# Other relevant\n",
    "import umap\n",
    "import statsmodels\n",
    "\n",
    "# Standard\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pr/c3wk8t5n3q14vnms13t5jjmh0000gn/T/ipykernel_39668/3154124639.py:3: DtypeWarning: Columns (70) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  ratios = pd.read_csv('../data/final_processed/firm_ratios.csv', parse_dates=['date']).sort_values(['date','ticker']).set_index('date')\n"
     ]
    }
   ],
   "source": [
    "# Read in dataset\n",
    "daily = pd.read_csv('../data/final_processed/daily_prices.csv', parse_dates=['date']).sort_values(['date','ticker']).set_index('date')\n",
    "ratios = pd.read_csv('../data/final_processed/firm_ratios.csv', parse_dates=['date']).sort_values(['date','ticker']).set_index('date')\n",
    "sectors = pd.read_csv('../data/final_processed/sectors.csv', parse_dates=['date']).sort_values(['date','ticker']).set_index('date')\n",
    "short = pd.read_csv('../data/final_processed/short_interest_rate.csv', parse_dates=['date']).sort_values(['date','ticker']).set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "df = daily.merge(ratios, on=['ticker', 'date'])\n",
    "df = df.merge(short, on =['ticker', 'date'])\n",
    "df = df.merge(sectors, on=['ticker', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formation periods of interes: quarter, 6 months, 1 year, 2 years\n",
    "df_q = df.loc['2005-01':'2005-03']\n",
    "to_drop = df_q.loc[df_q.isna().any(axis=1)]['ticker'].unique()\n",
    "df_q = df_q.loc[~df_q['ticker'].isin(to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index values and set up data set\n",
    "df_train = df_q.reset_index().sort_values(['ticker', 'date'])\n",
    "idx = df_train[['ticker', 'date']]\n",
    "df_train = df_train.drop(['date','ticker'], axis=1)\n",
    "\n",
    "# One hot encoding -> np array\n",
    "ohe_column = 'gicdesc'\n",
    "ohe_categories = df_train[ohe_column].unique().tolist()\n",
    "enc = OneHotEncoder(sparse_output=False, categories=[ohe_categories]) \n",
    "transformer = make_column_transformer((enc, [ohe_column]), remainder='passthrough') \n",
    "X_train = transformer.fit_transform(df_train)\n",
    "\n",
    "# PCA (should generalize this)\n",
    "num_components = 2\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "pca = PCA(num_components)\n",
    "components = pca.fit_transform(X_train)\n",
    "\n",
    "# Create a new df with components\n",
    "components_df = pd.DataFrame(data=components, columns=[f'pc {i+1}' for i in range(num_components)])\n",
    "\n",
    "# Merge the principal components with indices then groupby tickers\n",
    "merged_df = pd.concat([idx.reset_index(drop=True), components_df], axis=1)\n",
    "grouped_df = merged_df.groupby('ticker')\n",
    "\n",
    "# Concatenate the components for each ticker into a single vector\n",
    "vecs = {}\n",
    "for ticker, group in grouped_df:\n",
    "    components = group[[f'pc {i+1}' for i in range(num_components)]].values.T\n",
    "    vec_components = np.concatenate(components)\n",
    "    vecs[ticker] = vec_components\n",
    "\n",
    "# Create df with a single row for each ticker and vectorized components\n",
    "vectorized_df = pd.DataFrame(list(vecs.items()), columns=['ticker', 'components'])\n",
    "\n",
    "# Get mode of the vector lengths\n",
    "vector_lengths = [len(vector) for vector in vectorized_df['components']]\n",
    "lengths_series = pd.Series(vector_lengths)\n",
    "mode_length = lengths_series.mode().iloc[0]\n",
    "\n",
    "# Drop anything that doesn't have the right length components; should do it by mode of vector lengths\n",
    "for i, vector in enumerate(vectorized_df['components']):\n",
    "    if vector.shape != (mode_length,):\n",
    "        vectorized_df = vectorized_df.drop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      "0    [BBY, BC, CAR, DIS, GPC, GWW, HAS, HD, JWN, LE...\n",
      "1    [ABT, AGN, BAX, BCR, BDX, BMY, JNJ, LLY, MRK, ...\n",
      "2    [APA, APC, ASH, COP, CVX, HAL, KMG, MRO, OXY, ...\n",
      "3    [APD, ATI, BMS, CCK, CHA, ECL, EMN, FCX, IFF, ...\n",
      "4    [AEE, CMS, CNP, DUK, ED, EIX, ETR, EXC, OKE, P...\n",
      "5               [ADI, CAH, INTC, MCK, QCOM, TXN, XLNX]\n",
      "6       [AVP, BFO, CLX, COST, CVS, RAD, SVU, SYY, WMT]\n",
      "7    [AVY, BA, CAT, CCU, CMI, CR, DHR, DOV, EMR, FD...\n",
      "8                                 [ADP, GLW, IBM, XRX]\n",
      "9    [ADM, CAG, CL, CTB, DDS, DLX, GIS, GT, HSY, KM...\n",
      "Name: ticker, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielisaacgold/miniconda3/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Extract the vectors from the vectorized_df DataFrame\n",
    "vectors = np.array([np.array(vector) for vector in vectorized_df['components']])\n",
    "\n",
    "# K-means\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters)\n",
    "vectorized_df['cluster'] = kmeans.fit_predict(vectors)\n",
    "\n",
    "# Group the tickers by the assigned cluster labels\n",
    "clusters = vectorized_df.groupby('cluster')['ticker'].apply(list)\n",
    "print(clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
